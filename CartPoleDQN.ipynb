{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPA1bP90w/TomfTxxElw31",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achillesposiedon/TaxiGameRL/blob/master/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaIguJyEWP4t",
        "colab_type": "code",
        "outputId": "94f3fa99-dfdb-4191-f14a-36d1f95181bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLLp43CbVT9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "\n",
        "from keras.models import Sequential    \n",
        "from keras.layers import Dense, Flatten\n",
        "from collections import deque  \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaFczv2SWrB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v1') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycwrBr5qVuNN",
        "colab_type": "code",
        "outputId": "cd12e4a1-2071-4ca2-fa3e-d7803f87e4b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "model=Sequential()\n",
        "\n",
        "model.add(Dense(20, input_shape=(2,) + env.observation_space.shape, init='uniform', activation='relu'))\n",
        "model.add(Flatten())    \n",
        "model.add(Dense(18, init='uniform', activation='relu'))\n",
        "model.add(Dense(10, init='uniform', activation='relu'))\n",
        "model.add(Dense(env.action_space.n, init='uniform', activation='linear'))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, input_shape=(2, 4), activation=\"relu\", kernel_initializer=\"uniform\")`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(18, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, activation=\"linear\", kernel_initializer=\"uniform\")`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKF3cltMW9W9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2fFc-SyXwxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D=deque() #double queue \n",
        "time=5000\n",
        "epsilon=0.5\n",
        "gamma=0.9\n",
        "minibatchsize=500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze4SlDuwYK-k",
        "colab_type": "code",
        "outputId": "e20a9dbb-3986-4758-dfb3-54074835d53f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trail = env.reset()                     \n",
        "st1 = np.expand_dims(trail, axis=0)      \n",
        "state = np.stack((st1,st1), axis=1) # 2 times because we are using 2 consequtive states of the game for learning\n",
        "done = False\n",
        "for t in range(time):\n",
        "    if np.random.rand() <= epsilon: #Explore: take random action 60 percent of the time\n",
        "        action = np.random.randint(0, env.action_space.n, size=1)[0]\n",
        "    else:\n",
        "        Q = model.predict(state)          # Q-value predictions from Neural net\n",
        "        action = np.argmax(Q)             \n",
        "    st2=state[:,1]\n",
        "    \n",
        "    newstate, reward, done, info = env.step(action)     \n",
        "    newstate = np.expand_dims(newstate, axis=0)          \n",
        "    state_new = np.stack((newstate,st2), axis=1)\n",
        "    D.append((state, action, reward, state_new, done))         # 'Remember' action and consequence\n",
        "    state = state_new         # Update state\n",
        "    if done:\n",
        "        env.reset()           # Restart game if it's finished\n",
        "        obs = np.expand_dims(trail, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs \n",
        "        state = np.stack((st1, st1), axis=1)\n",
        "print('Exploring Done')\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exploring Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3KpzrvRcpQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Experience replay\n",
        "\n",
        "minibatch = random.sample(D, minibatchsize)                              # Sample some random moves from exploration memory\n",
        "for state, action, reward, state_next, terminal in minibatch:\n",
        "        q_update = reward\n",
        "        if not terminal:\n",
        "            q_update = (reward + gamma * np.amax(model.predict(state_next)[0]))\n",
        "        q_values = model.predict(state)\n",
        "        q_values[0][action] = q_update\n",
        "        model.fit(state, q_values, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF6sR3Edt1Ye",
        "colab_type": "code",
        "outputId": "eb8d68eb-4c54-441a-a2a4-19555bddddbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Play!\n",
        "\n",
        "observation = env.reset()\n",
        "rlst = np.expand_dims(observation, axis=0)\n",
        "state = np.stack((rlst, rlst), axis=1)\n",
        "done = False\n",
        "finalreward = 0.0\n",
        "while not done:\n",
        "    #env.render()   \n",
        "    st2=state[:,1]\n",
        "                    \n",
        "    Q = model.predict(state)        \n",
        "    action = np.argmax(Q)         \n",
        "    observation, reward, done, info = env.step(action)\n",
        "    obs = np.expand_dims(observation, axis=0)\n",
        "    state = np.stack((newstate,st2), axis=1)    \n",
        "    finalreward += reward\n",
        "print('Game ended! Total reward: {}'.format(finalreward))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Game ended! Total reward: 10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDKecgug9iiy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73e48c28-1142-4948-cbbe-0a903aa9cfe7"
      },
      "source": [
        "for i in range(1000):\n",
        "  trail = env.reset()                     \n",
        "  st1 = np.expand_dims(trail, axis=0)      \n",
        "  state = np.stack((st1,st1), axis=1) # 2 times because we are using 2 consequtive states of the game for learning\n",
        "  done = False\n",
        "  for t in range(time):\n",
        "    if np.random.rand() <= epsilon: #Explore: take random action 60 percent of the time\n",
        "        action = np.random.randint(0, env.action_space.n, size=1)[0]\n",
        "    else:\n",
        "        Q = model.predict(state)          # Q-value predictions from Neural net\n",
        "        action = np.argmax(Q)             \n",
        "    st2=state[:,1]\n",
        "    newstate, reward, done, info = env.step(action)     \n",
        "    newstate = np.expand_dims(newstate, axis=0)          \n",
        "    state_new = np.stack((newstate,st2), axis=1)\n",
        "    D.append((state, action, reward, state_new, done))         # 'Remember' action and consequence\n",
        "    state = state_new         # Update state\n",
        "    if done:\n",
        "        env.reset()           # Restart game if it's finished\n",
        "        obs = np.expand_dims(trail, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs \n",
        "        state = np.stack((st1, st1), axis=1)\n",
        "\n",
        "\n",
        "  minibatch = random.sample(D, minibatchsize)                              # Sample some random moves from exploration memory\n",
        "  for state, action, reward, state_next, terminal in minibatch:\n",
        "    q_update = reward\n",
        "    if not terminal:\n",
        "      q_update = (reward + gamma * np.amax(model.predict(state_next)[0]))\n",
        "    q_values = model.predict(state)\n",
        "    q_values[0][action] = q_update\n",
        "    model.fit(state, q_values, verbose=0)\n",
        "\n",
        "  observation = env.reset()\n",
        "  rlst = np.expand_dims(observation, axis=0)\n",
        "  state3 = np.stack((rlst, rlst), axis=1)\n",
        "  done = False\n",
        "  finalreward3 = 0.0\n",
        "  while not done:\n",
        "    #env.render()   \n",
        "    st23=state3[:,1]\n",
        "                    \n",
        "    Q3 = model.predict(state3)        \n",
        "    action3 = np.argmax(Q3)         \n",
        "    observation3, reward3, done, info = env.step(action3)\n",
        "    obs3 = np.expand_dims(observation3, axis=0)\n",
        "    state3 = np.stack((obs3,st2), axis=1)    \n",
        "    finalreward3 += reward3\n",
        "  print(finalreward3)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.0\n",
            "8.0\n",
            "9.0\n",
            "8.0\n",
            "8.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "9.0\n",
            "8.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "10.0\n",
            "9.0\n",
            "8.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "11.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "8.0\n",
            "10.0\n",
            "9.0\n",
            "11.0\n",
            "8.0\n",
            "10.0\n",
            "9.0\n",
            "15.0\n",
            "10.0\n",
            "8.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "8.0\n",
            "26.0\n",
            "10.0\n",
            "10.0\n",
            "11.0\n",
            "10.0\n",
            "12.0\n",
            "10.0\n",
            "9.0\n",
            "13.0\n",
            "18.0\n",
            "11.0\n",
            "10.0\n",
            "10.0\n",
            "61.0\n",
            "11.0\n",
            "10.0\n",
            "8.0\n",
            "13.0\n",
            "10.0\n",
            "22.0\n",
            "12.0\n",
            "52.0\n",
            "17.0\n",
            "10.0\n",
            "10.0\n",
            "31.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "16.0\n",
            "10.0\n",
            "13.0\n",
            "73.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "10.0\n",
            "14.0\n",
            "9.0\n",
            "13.0\n",
            "9.0\n",
            "8.0\n",
            "27.0\n",
            "167.0\n",
            "23.0\n",
            "13.0\n",
            "9.0\n",
            "19.0\n",
            "139.0\n",
            "14.0\n",
            "12.0\n",
            "21.0\n",
            "359.0\n",
            "13.0\n",
            "97.0\n",
            "227.0\n",
            "100.0\n",
            "355.0\n",
            "107.0\n",
            "21.0\n",
            "109.0\n",
            "50.0\n",
            "84.0\n",
            "122.0\n",
            "120.0\n",
            "93.0\n",
            "500.0\n",
            "126.0\n",
            "500.0\n",
            "500.0\n",
            "90.0\n",
            "95.0\n",
            "134.0\n",
            "26.0\n",
            "241.0\n",
            "13.0\n",
            "500.0\n",
            "128.0\n",
            "135.0\n",
            "122.0\n",
            "293.0\n",
            "139.0\n",
            "179.0\n",
            "459.0\n",
            "186.0\n",
            "181.0\n",
            "149.0\n",
            "128.0\n",
            "149.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50HXp5iTHOvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}